{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cc82b3ed-1c3d-4ba5-a846-304ffd977fee",
      "metadata": {
        "id": "cc82b3ed-1c3d-4ba5-a846-304ffd977fee"
      },
      "source": [
        "# The following Notebook is my attempt to build a Small Language Model, I have referred a few resources which i have attached in my Readme file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38cc2cc6-3f52-4d44-a2e5-04630c265081",
      "metadata": {
        "id": "38cc2cc6-3f52-4d44-a2e5-04630c265081"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f75ccdf-c302-41ed-96c6-181c37d4a529",
      "metadata": {
        "id": "6f75ccdf-c302-41ed-96c6-181c37d4a529"
      },
      "source": [
        "# I will use the TinyStories dataset to develop the small language model,\n",
        "# There ~2 Million stories (rows) and ~ 20,000 stories for validation, so the loading\n",
        "# might take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c896954-83ae-488b-9c6c-26d13505639d",
      "metadata": {
        "id": "7c896954-83ae-488b-9c6c-26d13505639d"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "df = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c337437d-94f7-4c3b-be03-6dd006fd68b0",
      "metadata": {
        "id": "c337437d-94f7-4c3b-be03-6dd006fd68b0"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9148fd-bf6a-45fd-ae38-d3dc7031c0e3",
      "metadata": {
        "id": "3f9148fd-bf6a-45fd-ae38-d3dc7031c0e3"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3133a974-d11c-4608-8c77-e1ce8801ed38",
      "metadata": {
        "id": "3133a974-d11c-4608-8c77-e1ce8801ed38"
      },
      "outputs": [],
      "source": [
        "type(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5e522b-6a77-48e0-a38d-25e87813f44b",
      "metadata": {
        "id": "4c5e522b-6a77-48e0-a38d-25e87813f44b"
      },
      "source": [
        "Step 1: We will use a tokenization scheme that is necessary before converting the data into a numeric form and then passing it to the SLM.\n",
        "We will use BPE (Byte Pair Encoding algorithm, a sub word tokenizer.....has many advantages over word or just character based tokenization approaches)...."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0833d980-415b-4b24-bcdb-7046c5cd35d2",
      "metadata": {
        "id": "0833d980-415b-4b24-bcdb-7046c5cd35d2"
      },
      "source": [
        "so every row corresponds to a story...and for every token in that row we will have a token id and it will be merged...\n",
        "We will store every token id in a .bin file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa8e876-61f9-410b-a1e7-5b1d5f92b610",
      "metadata": {
        "id": "1aa8e876-61f9-410b-a1e7-5b1d5f92b610"
      },
      "source": [
        ".bin file bcoz, it will get stored on the disk and the processing will be faster, since our data is too big (production level), it will avoid any sort of RAM overload and we can reuse the .bin file for training...also no need to retokenize again."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97ed73f-5736-4849-80c2-ddc0bc8fe5ec",
      "metadata": {
        "id": "b97ed73f-5736-4849-80c2-ddc0bc8fe5ec"
      },
      "source": [
        "We will also need to make sure that it is memory mapped using np.memmap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d1f6cb-785c-42ab-aba9-fb6c24f3e59f",
      "metadata": {
        "id": "e8d1f6cb-785c-42ab-aba9-fb6c24f3e59f"
      },
      "source": [
        "We will also do chunking, that is divide the data into batches and then store these batches in a train.bin file that will be present on the disk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ed389d-2870-41eb-b985-efd6a79ab6bd",
      "metadata": {
        "id": "06ed389d-2870-41eb-b985-efd6a79ab6bd"
      },
      "source": [
        "Batches makes the processing faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a3b586-2c2c-4a47-9893-8269ccd211c5",
      "metadata": {
        "id": "d5a3b586-2c2c-4a47-9893-8269ccd211c5"
      },
      "outputs": [],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68244ab7-0838-48f7-92fd-239ac8d79db0",
      "metadata": {
        "id": "68244ab7-0838-48f7-92fd-239ac8d79db0"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ad0a97-94b3-4b2f-875f-50809368da47",
      "metadata": {
        "id": "27ad0a97-94b3-4b2f-875f-50809368da47"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deef1c94-c812-470c-9ea2-429a79b55817",
      "metadata": {
        "id": "deef1c94-c812-470c-9ea2-429a79b55817"
      },
      "outputs": [],
      "source": [
        "def processing(sample_text):\n",
        "    ids = encoding.encode_ordinary(sample_text['text'])\n",
        "    out = {'ids':ids,'len':len(ids)}\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6a8e1b-5bca-4c47-9929-020c7b533c5a",
      "metadata": {
        "id": "0e6a8e1b-5bca-4c47-9929-020c7b533c5a"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = df.map(\n",
        "        processing, # our token processing function defined above....basically we are mapping teh data here...:)\n",
        "        remove_columns=['text'],\n",
        "        desc=\"tokenizing the splits\",\n",
        "        num_proc=8,\n",
        "        )\n",
        "    for split, dset in tokenized.items():\n",
        "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "        filename = f'{split}.bin'\n",
        "        dtype = np.uint16 # this tells you the total bits, so 16 here so 2^16 possible tokens which very well fits our training data\n",
        "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "        total_batches = 1024\n",
        "        idx = 0\n",
        "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "            # We will Batch together samples for faster write\n",
        "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "            arr_batch = np.concatenate(batch['ids'])\n",
        "            # Here we write into mmap\n",
        "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "            idx += len(arr_batch)\n",
        "        arr.flush()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1a3f48-5502-443e-a0c2-bfd968a6e3d5",
      "metadata": {
        "id": "7c1a3f48-5502-443e-a0c2-bfd968a6e3d5"
      },
      "source": [
        "Now we will have to creat input output pairs..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45817278-082b-4c9f-8e04-2f5cd4d4364a",
      "metadata": {
        "id": "45817278-082b-4c9f-8e04-2f5cd4d4364a"
      },
      "source": [
        "We will now have to define following things first..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b759fd7-238e-4245-9bd7-f5476a024c3a",
      "metadata": {
        "id": "5b759fd7-238e-4245-9bd7-f5476a024c3a"
      },
      "source": [
        "Context size (what the slm sees before predicting the next token)\n",
        "we will use the context_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc272d8d-9806-43af-965f-999e929c3d8d",
      "metadata": {
        "id": "bc272d8d-9806-43af-965f-999e929c3d8d"
      },
      "source": [
        "After this stage the model has no idea about the raws words it will just see the token IDs, so if they are like [23,43,56,34,7,3,....], the ontext size of 4 means that the model will see [23,43,56,34] and predict the next token as [7], this will be done across all the tokens of train.bin file, which i think will probably have more than 100 million training ids on record!!!, we will not play around with strides, each step will the output as one placed of the input token shifted by 1 in the actual train.bin file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e037d6-aa4d-46a9-8ff6-cb292a4d8203",
      "metadata": {
        "id": "b2e037d6-aa4d-46a9-8ff6-cb292a4d8203"
      },
      "source": [
        "Now wee need batches, batch size = 6 here, to reduce the time to update the entire params while backpropagating"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a33a1a5-11b5-4b83-9052-0bdd9d20bcba",
      "metadata": {
        "id": "7a33a1a5-11b5-4b83-9052-0bdd9d20bcba"
      },
      "source": [
        "Now we have an input tensor and an output tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152ec24e-0011-4598-a1c2-71ad283d8246",
      "metadata": {
        "id": "152ec24e-0011-4598-a1c2-71ad283d8246"
      },
      "source": [
        "Since context size is 4, for each row in the input and output tensor, we have four prediction tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ac7ae3-bd56-4787-bcab-d88d96d1a04a",
      "metadata": {
        "id": "98ac7ae3-bd56-4787-bcab-d88d96d1a04a"
      },
      "source": [
        "For example: X1 = [23,4,2,66] and y1 = [4,2,66,8], for this i/p o/p pair, we are esentially predicting 4 sentences,i.e, if 23 is input, 4 is the output, if 23,4 is input then 2 is output, if 23,4,2 is input then 66 is output and if 23,4,2,66 i input then 8 is the output..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2793f0ff-b431-4526-bb9d-c55c4d547f37",
      "metadata": {
        "id": "2793f0ff-b431-4526-bb9d-c55c4d547f37"
      },
      "source": [
        "What i am essentially going to do here is create random batches, meaning X1,X2....etc can be from anywhere in the originial data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60cb7d4d-6a66-47db-8b1e-2a19efd5c6f4",
      "metadata": {
        "id": "60cb7d4d-6a66-47db-8b1e-2a19efd5c6f4"
      },
      "source": [
        "## I will also implement Memory Locking so that we have reserved memory in the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c152fae0-78f5-4e0d-bd93-6224e42eef62",
      "metadata": {
        "id": "c152fae0-78f5-4e0d-bd93-6224e42eef62"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ae8477-36cd-44ef-b23c-94c45a24a128",
      "metadata": {
        "id": "d8ae8477-36cd-44ef-b23c-94c45a24a128"
      },
      "source": [
        "in the above cell, we are stacking x1,x2....etc into x, ix is the random id of the batches which i have created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e119fe00-51e4-4266-825a-bd09f76b729c",
      "metadata": {
        "id": "e119fe00-51e4-4266-825a-bd09f76b729c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5899c734-d51b-4ba1-b805-b6c4615e961a",
      "metadata": {
        "id": "5899c734-d51b-4ba1-b805-b6c4615e961a"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb8428a-db1b-4440-b8e3-d211d751d9e1",
      "metadata": {
        "id": "1fb8428a-db1b-4440-b8e3-d211d751d9e1"
      },
      "source": [
        "NOTE: It is important to have Layer Normalization, bcoz of several reasons, some of them i have mentioned in my blog here while trying to explain the DyTs (Transformers without Normalization) check out here :https://medium.com/@kakadechaitanya77/what-exactly-is-transformers-without-normalization-dyt-part-1-3cdeae976c00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25074706-89c0-48f0-a263-cfad629bbc92",
      "metadata": {
        "id": "25074706-89c0-48f0-a263-cfad629bbc92"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                       .view(1, 1, config.block_size, config.block_size))\n",
        "#This part of the code, i have introduced\n",
        "#the attention mechanism of the transformer architecture...\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "\n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69547e71-58ef-427e-986a-b811d0b3e8a8",
      "metadata": {
        "id": "69547e71-58ef-427e-986a-b811d0b3e8a8"
      },
      "source": [
        "Following is the MLP architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff8bce9-2097-4962-8c69-e8af83a788c9",
      "metadata": {
        "id": "0ff8bce9-2097-4962-8c69-e8af83a788c9"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa2d605-9883-46b1-a285-5d6e1de490d7",
      "metadata": {
        "id": "6fa2d605-9883-46b1-a285-5d6e1de490d7"
      },
      "source": [
        "Now ,  I will create the Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e75443-7d68-43b5-a772-e6b63e5c00f9",
      "metadata": {
        "id": "90e75443-7d68-43b5-a772-e6b63e5c00f9"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3c75f4-9070-4177-ad10-e6b725a5d305",
      "metadata": {
        "id": "cb3c75f4-9070-4177-ad10-e6b725a5d305"
      },
      "source": [
        "x = x + self.attn(self.ln1(x))\n",
        "x = x + self.mlp(self.ln2(x))\n",
        "If you analyse this two lines deeply, these are basically the residual connections or skip connections (as in the ResNet architecture), They will help the past gradients flow into the future, so that meaning or importance (long term dependency is not lost) Think of the vanishing gradient issues that can prevent the NN to larn from the previous deep inputs and weight updation slows...thereby increasing the training time...!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95f97d4-8339-4d8d-96cd-1276a29de712",
      "metadata": {
        "id": "d95f97d4-8339-4d8d-96cd-1276a29de712"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)   #this is out logits matrix which i have explaine below...\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1) # the loss is cross entropy\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate tokens given a conditioning sequence.\n",
        "        idx: Tensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4af0bc-5234-49c1-b558-140670cdb1dd",
      "metadata": {
        "id": "dd4af0bc-5234-49c1-b558-140670cdb1dd"
      },
      "source": [
        "# Now we will lool at how the output is actually computed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0608e74-abde-4360-aacf-5ae9012c9420",
      "metadata": {
        "id": "e0608e74-abde-4360-aacf-5ae9012c9420"
      },
      "source": [
        "So the output of our transformer block is 4*768, where 4 is the context size and 768 is the vector dimension of the embeddings given as the lm_head in the code above...(which is also caled as the logits matrix)....\n",
        "\n",
        "Note: This logits matrix gets used for the next token/word prediction task....\n",
        "\n",
        "Now what we want is to predict the next token! How do we do that from the logits matrix??\n",
        "so, i know that eaxh of the batch itself has 4 prediction tasks as explained in some cell above... every token in the batch has now an output dimension equal to the vocab size (Think of the logits matrix and rows and columns, where rows size is 4 as that was our context size and the columns lenght is vacab size, eaxh row is the token)so if we look at the first token, we will see VOCABSIZE number of probability values and the one with highes prob is the corresponding next token in as indexes by the vocabulary..... WOW!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f33dc7-d64a-45c0-982c-9ccf204597e9",
      "metadata": {
        "id": "c5f33dc7-d64a-45c0-982c-9ccf204597e9"
      },
      "source": [
        "We then compare the output values with original batch and then compute loss and we want to minimize this loss then....We use BackPropagation mechanism!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22273c3b-096b-4be3-8e79-a0530ed74ae9",
      "metadata": {
        "id": "22273c3b-096b-4be3-8e79-a0530ed74ae9"
      },
      "source": [
        "\n",
        "\n",
        "There will be 4 losses as the batch size we have taken is 4, so let these individual losses be L1,L2,L3 and L4, so the net Loss is L1+L2+L3+L4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf854d6-ed25-47ad-8c96-00dd773b89f6",
      "metadata": {
        "id": "6bf854d6-ed25-47ad-8c96-00dd773b89f6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " I have also done some Initializations which are mentioned as below:\n",
        "These are basically the trainable params:\n",
        "Token embedding layer (wte)\n",
        "pos embedding layer (wpe)\n",
        "1st attention block (layerNorm before)\n",
        "QKV linear (c_attn)\n",
        "output c_proj\n",
        "2nd attention block (Layer norm before MLP)\n",
        "MLP block (c_fc)\n",
        "output c_proj\n",
        "final layer norm\n",
        "output head lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdae0bb-3569-4d24-be5a-05dca1333743",
      "metadata": {
        "id": "9fdae0bb-3569-4d24-be5a-05dca1333743"
      },
      "outputs": [],
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,\n",
        "    block_size=128,       # here i have taken the context size of 128, i used 4 only for explanation\n",
        "    n_layer=6, # no of layer is the transformer blocks' number that u use...\n",
        "    n_head=6, #no of attention heads as in Multi Head Attention\n",
        "    n_embd=384, # The embedding dimension .... try chnaging thinhs in this code here and there\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be174888-b920-42bb-94eb-64976bd2b9a7",
      "metadata": {
        "id": "be174888-b920-42bb-94eb-64976bd2b9a7"
      },
      "source": [
        "The loss function is essentially cross entropy (i.e negative log likelihood), we see the probability values of the correct output token in the logits matrix\n",
        "and we want that value to be as close to 1 as possible\n",
        "so for a batch of size four, we will have original output p1,p2,p3 and p4 whose values must be 1 iteratively and the predicted probabilities this will be for each item in batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53efca0b-52fb-4f3e-a2f1-22cd21e7379b",
      "metadata": {
        "id": "53efca0b-52fb-4f3e-a2f1-22cd21e7379b"
      },
      "outputs": [],
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8965dbb2-b031-47d4-9636-a2d1a7b78d4a",
      "metadata": {
        "id": "8965dbb2-b031-47d4-9636-a2d1a7b78d4a"
      },
      "source": [
        "Note : I will be using the AMP ( Automatic Mixed Precision Method) That will automatically decide which Floating Point Precision to be used\n",
        "This will dynamically make the processing faster and overall improved efficiency!\n",
        "For example: when any matrix calculations are happening, then it will use FP16 and if any softmax computation is happening then it will use FP32 as we are cmputing exponentiation operation and we dont want any overflow/underflow error ocurring..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23eedc3f-1597-4c96-b849-e51a0fe56b20",
      "metadata": {
        "id": "23eedc3f-1597-4c96-b849-e51a0fe56b20"
      },
      "source": [
        "# Training Loop/Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460dd08a-8404-4667-b825-99645f670e86",
      "metadata": {
        "id": "460dd08a-8404-4667-b825-99645f670e86"
      },
      "outputs": [],
      "source": [
        "# Training Config\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        "max_iters = 10000 #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d642273d-3092-4d80-a13f-69b59d295d03",
      "metadata": {
        "id": "d642273d-3092-4d80-a13f-69b59d295d03"
      },
      "source": [
        "here i am accumulating the gradients after 32 iterations and then will update the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b32e845-4912-4aa2-b571-8d725c22c343",
      "metadata": {
        "id": "6b32e845-4912-4aa2-b571-8d725c22c343"
      },
      "source": [
        "I am going to use the ADAM optimizer with weight decay, annd the learning rate initially is linear in nature and then becomes cosine like (warmup and decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f64d17-5f3a-4c86-bc20-78300e3d26d5",
      "metadata": {
        "id": "76f64d17-5f3a-4c86-bc20-78300e3d26d5"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we wll start training the model (Pre Training)"
      ],
      "metadata": {
        "id": "DL-s-65q0HVE"
      },
      "id": "DL-s-65q0HVE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53fdef4f-ebc7-4c85-9363-2fee3c8dd762",
      "metadata": {
        "id": "53fdef4f-ebc7-4c85-9363-2fee3c8dd762"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I trained it on T4 GPU, took ~ 2hours training only for 10000 epochs, if i train it longer, its output will be better..."
      ],
      "metadata": {
        "id": "hoz8CnGgMYVC"
      },
      "id": "hoz8CnGgMYVC"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
        "plt.xlabel(\"Steps - Every 100 epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c_3j0PuV0R-L"
      },
      "id": "c_3j0PuV0R-L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "model = GPT(config)  # re-create the model with same config\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
      ],
      "metadata": {
        "id": "79hxRXHIMQVx"
      },
      "id": "79hxRXHIMQVx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Once upon a time there was a girl.\"\n",
        "context = (torch.tensor(encoding.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(encoding.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "id": "Re3jQl7XMox3"
      },
      "id": "Re3jQl7XMox3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A little girl went to the woods\"\n",
        "context = (torch.tensor(encoding.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
        "y = model.generate(context, 200)\n",
        "print(encoding.decode(y.squeeze().tolist()))"
      ],
      "metadata": {
        "id": "CnAHXnWsNRzn"
      },
      "id": "CnAHXnWsNRzn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XCJ0G9j7MX3U"
      },
      "id": "XCJ0G9j7MX3U"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}